{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/espada105/NewRecipe/blob/main/Llama3_2_11B_Vision_Instruct_NewRecipe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTWbtWgeZLmt"
   },
   "source": [
    "#라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tI0S-66dgIQW",
    "outputId": "2fa30dc8-69e5-4972-e8cc-f55a741e954e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: transformers in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (4.46.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from transformers) (4.66.6)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-18.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.10-cp311-cp311-win_amd64.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from accelerate) (6.1.0)\n",
      "Collecting torch>=1.10.0 (from accelerate)\n",
      "  Downloading torch-2.5.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.17.1-cp311-cp311-win_amd64.whl.metadata (66 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Collecting networkx (from torch>=1.10.0->accelerate)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Collecting sympy==1.13.1 (from torch>=1.10.0->accelerate)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.10.0->accelerate)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets)\n",
      "  Downloading propcache-0.2.0-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Downloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading aiohttp-3.10.10-cp311-cp311-win_amd64.whl (381 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-18.0.0-cp311-cp311-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.4/25.1 MB 11.2 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 4.2/25.1 MB 10.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 6.3/25.1 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.4/25.1 MB 10.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.7/25.1 MB 10.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 13.1/25.1 MB 10.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.5/25.1 MB 10.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 16.3/25.1 MB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.3/25.1 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 17.8/25.1 MB 8.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.6/25.1 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.9/25.1 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.8/25.1 MB 8.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/25.1 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 8.3 MB/s eta 0:00:00\n",
      "Downloading torch-2.5.1-cp311-cp311-win_amd64.whl (203.1 MB)\n",
      "   ---------------------------------------- 0.0/203.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/203.1 MB 12.2 MB/s eta 0:00:17\n",
      "    --------------------------------------- 4.7/203.1 MB 11.9 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 7.1/203.1 MB 12.1 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 7.9/203.1 MB 9.7 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 10.0/203.1 MB 9.9 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 12.3/203.1 MB 10.2 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 14.7/203.1 MB 10.4 MB/s eta 0:00:19\n",
      "   --- ------------------------------------ 17.0/203.1 MB 10.5 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 19.4/203.1 MB 10.6 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 21.5/203.1 MB 10.5 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 23.3/203.1 MB 10.5 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 25.2/203.1 MB 10.4 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 27.3/203.1 MB 10.3 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 28.6/203.1 MB 10.1 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 30.4/203.1 MB 10.0 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 32.8/203.1 MB 10.1 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 34.6/203.1 MB 10.1 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 35.7/203.1 MB 9.7 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 38.0/203.1 MB 9.8 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 40.4/203.1 MB 9.9 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 42.7/203.1 MB 10.0 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 45.1/203.1 MB 10.1 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 47.4/203.1 MB 10.2 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 50.1/203.1 MB 10.3 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 52.4/203.1 MB 10.3 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 55.1/203.1 MB 10.4 MB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 57.1/203.1 MB 10.4 MB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 59.2/203.1 MB 10.4 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 61.3/203.1 MB 10.4 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 63.7/203.1 MB 10.4 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 65.5/203.1 MB 10.4 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 67.6/203.1 MB 10.4 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 69.7/203.1 MB 10.4 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 72.4/203.1 MB 10.4 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 74.7/203.1 MB 10.4 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 77.1/203.1 MB 10.5 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 79.4/203.1 MB 10.5 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 81.8/203.1 MB 10.6 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 83.9/203.1 MB 10.6 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 86.2/203.1 MB 10.6 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 88.9/203.1 MB 10.6 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 91.2/203.1 MB 10.7 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 93.6/203.1 MB 10.7 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 95.9/203.1 MB 10.7 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 98.3/203.1 MB 10.7 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 100.7/203.1 MB 10.7 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 102.2/203.1 MB 10.7 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 104.6/203.1 MB 10.7 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 107.0/203.1 MB 10.7 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 109.3/203.1 MB 10.7 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 111.7/203.1 MB 10.8 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 113.8/203.1 MB 10.7 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 115.6/203.1 MB 10.7 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 117.7/203.1 MB 10.7 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 120.1/203.1 MB 10.7 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 122.4/203.1 MB 10.7 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 123.5/203.1 MB 10.6 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 125.6/203.1 MB 10.6 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 128.2/203.1 MB 10.6 MB/s eta 0:00:08\n",
      "   ------------------------- ------------- 130.5/203.1 MB 10.7 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 132.9/203.1 MB 10.7 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 135.5/203.1 MB 10.7 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 137.9/203.1 MB 10.7 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 140.5/203.1 MB 10.7 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 142.6/203.1 MB 10.8 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 143.9/203.1 MB 10.7 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 146.3/203.1 MB 10.7 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 148.6/203.1 MB 10.7 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 151.0/203.1 MB 10.7 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 153.6/203.1 MB 10.7 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 156.0/203.1 MB 10.8 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 158.3/203.1 MB 10.8 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 160.7/203.1 MB 10.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 163.1/203.1 MB 10.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 165.4/203.1 MB 10.8 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 168.0/203.1 MB 10.8 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 169.9/203.1 MB 10.8 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 171.7/203.1 MB 10.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 174.1/203.1 MB 10.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 176.4/203.1 MB 10.8 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 178.5/203.1 MB 10.8 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 180.4/203.1 MB 10.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 182.5/203.1 MB 10.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 184.0/203.1 MB 10.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 185.3/203.1 MB 10.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 187.4/203.1 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 189.0/203.1 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 191.1/203.1 MB 10.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 193.2/203.1 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 195.3/203.1 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  197.9/203.1 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  200.3/203.1 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.6/203.1 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------------------------------- 203.1/203.1 MB 10.6 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 2.4/6.2 MB 12.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.2 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 11.5 MB/s eta 0:00:00\n",
      "Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Downloading yarl-1.17.1-cp311-cp311-win_amd64.whl (90 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 10.4 MB/s eta 0:00:00\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading propcache-0.2.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Installing collected packages: pytz, mpmath, xxhash, tzdata, sympy, pyarrow, propcache, networkx, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, torch, pandas, multiprocess, aiosignal, aiohttp, accelerate, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "Successfully installed accelerate-1.1.1 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 attrs-24.2.0 datasets-3.1.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.9.0 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 networkx-3.4.2 pandas-2.2.3 propcache-0.2.0 pyarrow-18.0.0 pytz-2024.2 sympy-1.13.1 torch-2.5.1 tzdata-2024.2 xxhash-3.5.0 yarl-1.17.1\n",
      "Requirement already satisfied: llama-stack in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (0.0.49)\n",
      "Requirement already satisfied: blobfile in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from llama-stack) (3.0.0)\n",
      "Requirement already satisfied: fire in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from llama-stack) (0.7.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from llama-stack) (0.27.2)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from llama-stack) (0.26.2)\n",
      "Requirement already satisfied: llama-models>=0.0.49 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from llama-stack) (0.0.49)\n",
      "Requirement already satisfied: prompt-toolkit in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from llama-stack) (3.0.48)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from llama-stack) (1.0.1)\n",
      "Requirement already satisfied: pydantic>=2 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from llama-stack) (2.9.2)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from llama-stack) (2.32.3)\n",
      "Requirement already satisfied: rich in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from llama-stack) (13.9.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from llama-stack) (75.3.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from llama-stack) (2.5.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from llama-models>=0.0.49->llama-stack) (6.0.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from llama-models>=0.0.49->llama-stack) (3.1.4)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from llama-models>=0.0.49->llama-stack) (0.8.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from llama-models>=0.0.49->llama-stack) (11.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from pydantic>=2->llama-stack) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from pydantic>=2->llama-stack) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from pydantic>=2->llama-stack) (4.12.2)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from blobfile->llama-stack) (3.21.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from blobfile->llama-stack) (2.2.3)\n",
      "Requirement already satisfied: lxml>=4.9 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from blobfile->llama-stack) (5.3.0)\n",
      "Requirement already satisfied: filelock>=3.0 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from blobfile->llama-stack) (3.16.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from httpx->llama-stack) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from httpx->llama-stack) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from httpx->llama-stack) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from httpx->llama-stack) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from httpx->llama-stack) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-stack) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from huggingface-hub->llama-stack) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from huggingface-hub->llama-stack) (24.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from huggingface-hub->llama-stack) (4.66.6)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from prompt-toolkit->llama-stack) (0.2.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from requests->llama-stack) (3.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from rich->llama-stack) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from rich->llama-stack) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->llama-stack) (0.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->llama-stack) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from jinja2->llama-models>=0.0.49->llama-stack) (3.0.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\pc\\documents\\github\\computervision-and-deep-learning\\newrecipe\\.venv\\lib\\site-packages (from tiktoken->llama-models>=0.0.49->llama-stack) (2024.9.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers\n",
    "!pip install transformers datasets accelerate\n",
    "!pip install llama-stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DqZnNN9PZ3AM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Documents\\GitHub\\ComputerVision-and-Deep-learning\\NewRecipe\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWST8Po1kl2T"
   },
   "source": [
    "#디바이스 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vV-bluX8koZo"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5zw0E_sbkc7"
   },
   "source": [
    "#허깅페이스 로그인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rl1XkPF9bn_Q",
    "outputId": "1ae18be2-76ca-4f16-e78b-e9d3ecb7a657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) y\n",
      "Token is valid (permission: fineGrained).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nh7Hvbsfbyur"
   },
   "source": [
    "#모델 및 프로세서 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "ICHh6sCqeJDT",
    "outputId": "4ca98814-54f0-43ec-ec72-6b5ef55db0d1"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0bfa75cfc3fb>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 모델과 프로세서를 8비트 양자화로 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta-llama/Llama-3.2-11B-Vision-Instruct\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m model = AutoModelForImageTextToText.from_pretrained(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"meta-llama/Llama-3.2-11B-Vision-Instruct\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantization_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3657\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3658\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_flax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3659\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m             )\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             )\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 8비트 양자화 설정\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "# 모델과 프로세서를 8비트 양자화로 로드\n",
    "processor = AutoProcessor.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision-Instruct\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"  # 자동 장치 매핑 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygEnKIsSZuUd"
   },
   "source": [
    "#모델 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QZOPuRUaAcN"
   },
   "source": [
    "#레시피 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "05fac40a5dc14f26b1850b404d71ec8b",
      "f529571adec6429881501abd7823a29d",
      "382fa38dd8dd432d8ac637e2cd3e610b",
      "27349919ad414291b3252d08143ff882",
      "49ea66600c4341d1b389fb44ec6f527a",
      "b0fc03076740425e8bc6e61ec3bd08d2",
      "911ba902843943f596914fadd8a9dcf3",
      "d8a93942a6aa42d5b635cf44448810ca",
      "3f118e20aeda4192a1ef42fd0e12f757",
      "2925a58c71a0463285cd39f938507640",
      "c3eb36a6e72f4afd9a42543cf06e2d4c"
     ]
    },
    "id": "R5QeILnB2ba8",
    "outputId": "2aae93a5-f302-4603-f53a-aa7c26ade042"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fac40a5dc14f26b1850b404d71ec8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = \"/content/drive/MyDrive/NewRecipe/full_dataset.csv\"\n",
    "\n",
    "recipe_dataset = load_dataset(\"csv\", data_files=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qkyTh_A2hEU",
    "outputId": "b21ee064-944b-4969-a068-d6f358efbf74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Unnamed: 0', 'title', 'ingredients', 'directions', 'link', 'source', 'NER'],\n",
      "    num_rows: 5\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(recipe_dataset['train'].select(range(5)))  # 처음 5개 행 출력 (데이터 로드 확인용임)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "jidRgxM0aGID"
   },
   "outputs": [],
   "source": [
    "# 레시피 데이터셋에서 필수 정보 확인 (재료와 레시피 이름)\n",
    "def get_relevant_recipes(ingredients):\n",
    "    relevant_recipes = []\n",
    "\n",
    "    for recipe in recipe_dataset[\"train\"]:\n",
    "        # 레시피에서 재료 정보 가져오기\n",
    "        recipe_ingredients = recipe.get(\"ingredients\", \"\").lower()\n",
    "\n",
    "        # 모든 입력된 재료가 레시피에 포함되는지 확인\n",
    "        if all(ingredient in recipe_ingredients for ingredient in ingredients):\n",
    "            relevant_recipes.append({\n",
    "                \"title\": recipe.get(\"title\", \"No Title\"),              # 레시피 제목\n",
    "                \"ingredients\": recipe_ingredients,                      # 레시피 재료\n",
    "                \"instructions\": recipe.get(\"instructions\", \"No Instructions\")  # 레시피 조리법\n",
    "            })\n",
    "\n",
    "    return relevant_recipes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q70pOPKraJFx"
   },
   "source": [
    "#이미지에서 식재료 객체 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "hlEQexVsaOnv",
    "outputId": "a564ddc7-bf67-4250-ab8b-96f86b855375"
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 39.56 GiB of which 834.81 MiB is free. Process 60209 has 38.74 GiB memory in use. Of the allocated memory 37.99 GiB is allocated by PyTorch, and 268.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-9468f0a48d3e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 모델과 프로세서를 device로 이동\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecognize_ingredients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3155\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3156\u001b[0m                 )\n\u001b[0;32m-> 3157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 39.56 GiB of which 834.81 MiB is free. Process 60209 has 38.74 GiB memory in use. Of the allocated memory 37.99 GiB is allocated by PyTorch, and 268.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "def recognize_ingredients(image_path):\n",
    "    # 이미지 열기 및 RGB로 변환\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # 이미지 전처리 후 텐서를 'device'로 이동\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # 모든 입력을 동일한 'device'로 이동\n",
    "\n",
    "    # 모델 출력 생성 (max_new_tokens 설정 추가)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)  # max_new_tokens로 출력 길이 조정\n",
    "\n",
    "    # 결과 디코딩 (디코딩 전에 CPU로 이동)\n",
    "    ingredients = processor.tokenizer.decode(outputs[0].cpu(), skip_special_tokens=True)  # .tokenizer 사용\n",
    "    ingredients_list = ingredients.lower().split(\", \")\n",
    "\n",
    "    return ingredients_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmHp9_mkaPrh"
   },
   "source": [
    "# 이미지로부터 식재료 인식 및 레시피 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "_T-_ZbP9XMZs"
   },
   "outputs": [],
   "source": [
    "def get_recipe_recommendations(image_path):\n",
    "    # 이미지에서 식재료 인식\n",
    "    recognized_ingredients = recognize_ingredients(image_path)\n",
    "    print(f\"Recognized ingredients: {recognized_ingredients}\")\n",
    "\n",
    "    # 인식된 식재료를 바탕으로 레시피 검색\n",
    "    recipes = get_relevant_recipes(recognized_ingredients)\n",
    "    if recipes:\n",
    "        print(\"Recommended Recipes:\")\n",
    "        for i, recipe in enumerate(recipes[:5]):  # 상위 5개 레시피 추천\n",
    "            print(f\"\\nRecipe {i+1}: {recipe['title']}\")\n",
    "            print(f\"Ingredients: {recipe['ingredients']}\")\n",
    "            print(f\"Instructions: {recipe['instructions']}\")\n",
    "    else:\n",
    "        print(\"No recipes found for the given ingredients.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SA-NBwltaUjb"
   },
   "source": [
    "\n",
    "# 예제 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "0KDbyBnIaV27",
    "outputId": "26c6a169-9dde-4402-9086-a05543e5f317"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-2f2fb323b863>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/NewRecipe/무.jpg\"\u001b[0m  \u001b[0;31m# 입력할 이미지 경로 -> 이건 우리가 테스트할 이미지 넣으면 됨 ㅇㅇ..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_recipe_recommendations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recognized ingredients:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingredients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-44f27c4cdd05>\u001b[0m in \u001b[0;36mget_recipe_recommendations\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_recipe_recommendations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# 이미지에서 식재료 인식\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrecognized_ingredients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecognize_ingredients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Recognized ingredients: {recognized_ingredients}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-80337bde1c69>\u001b[0m in \u001b[0;36mrecognize_ingredients\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# 모델 출력 생성 (max_new_tokens 설정 추가)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# max_new_tokens로 출력 길이 조정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# 결과 디코딩 (디코딩 전에 CPU로 이동)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2216\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mllama/modeling_mllama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, aspect_ratio_mask, aspect_ratio_ids, attention_mask, cross_attention_mask, cross_attention_states, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   2105\u001b[0m             )\n\u001b[1;32m   2106\u001b[0m             \u001b[0mcross_attention_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvision_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2107\u001b[0;31m             cross_attention_states = self.multi_modal_projector(cross_attention_states).reshape(\n\u001b[0m\u001b[1;32m   2108\u001b[0m                 \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_attention_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "image_path = \"/content/drive/MyDrive/NewRecipe/무.jpg\"  # 입력할 이미지 경로 -> 이건 우리가 테스트할 이미지 넣으면 됨 ㅇㅇ..\n",
    "get_recipe_recommendations(image_path)\n",
    "print(\"Recognized ingredients:\", ingredients)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN/XA6C6ekKyzayVUQRcFbi",
   "gpuType": "A100",
   "include_colab_link": true,
   "mount_file_id": "1cKOaRTrj8_A_sZoQRaOCxYQ5NjMPO4TI",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05fac40a5dc14f26b1850b404d71ec8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f529571adec6429881501abd7823a29d",
       "IPY_MODEL_382fa38dd8dd432d8ac637e2cd3e610b",
       "IPY_MODEL_27349919ad414291b3252d08143ff882"
      ],
      "layout": "IPY_MODEL_49ea66600c4341d1b389fb44ec6f527a"
     }
    },
    "27349919ad414291b3252d08143ff882": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2925a58c71a0463285cd39f938507640",
      "placeholder": "​",
      "style": "IPY_MODEL_c3eb36a6e72f4afd9a42543cf06e2d4c",
      "value": " 2231142/0 [01:07&lt;00:00, 34753.76 examples/s]"
     }
    },
    "2925a58c71a0463285cd39f938507640": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "382fa38dd8dd432d8ac637e2cd3e610b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8a93942a6aa42d5b635cf44448810ca",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3f118e20aeda4192a1ef42fd0e12f757",
      "value": 1
     }
    },
    "3f118e20aeda4192a1ef42fd0e12f757": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "49ea66600c4341d1b389fb44ec6f527a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "911ba902843943f596914fadd8a9dcf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0fc03076740425e8bc6e61ec3bd08d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3eb36a6e72f4afd9a42543cf06e2d4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8a93942a6aa42d5b635cf44448810ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "f529571adec6429881501abd7823a29d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0fc03076740425e8bc6e61ec3bd08d2",
      "placeholder": "​",
      "style": "IPY_MODEL_911ba902843943f596914fadd8a9dcf3",
      "value": "Generating train split: "
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
